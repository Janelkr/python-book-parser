import requests
import pandas as pd
import time
import random

CATEGORY = "fiction"  
BASE_URL = f"https://openlibrary.org/subjects/{CATEGORY}.json?limit=100&offset="
MAX_PAGES = 50
DELAY_RANGE = (1, 3)  
MAX_RUBRICS = 7  

RESULTS = []

def debug_print(data, label=""):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    if isinstance(data, dict):
        print(f"üîç {label} JSON —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–∏:", list(data.keys()))
    else:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ {label}: JSON –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—ë–º")

def get_best_edition(book_key):
    """–ë–µ—Ä—ë–º —Å–∞–º–æ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∏–∑–¥–∞–Ω–∏–µ"""
    editions_url = f"https://openlibrary.org{book_key}/editions.json"
    editions_response = requests.get(editions_url)

    if editions_response.status_code != 200:
        return None

    editions_data = editions_response.json()
    if "entries" not in editions_data or not editions_data["entries"]:
        return None

    best_edition = max(editions_data["entries"], key=lambda e: len(e.keys()))
    debug_print(best_edition, "BEST EDITION")
    return best_edition

def get_book_details(book_key):
    """–ü–æ–ª—É—á–∞–µ–º –¥–æ–ø. –∏–Ω—Ñ–æ: —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —è–∑—ã–∫, –∏–∑–¥–∞—Ç–µ–ª—è, –º–µ—Å—Ç–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
    if not book_key:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    url = f"https://openlibrary.org{book_key}.json"
    response = requests.get(url)

    if response.status_code != 200:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    data = response.json()
    debug_print(data, "WORK")  

    # ‚úÖ 1. –ë–µ—Ä—ë–º —è–∑—ã–∫
    languages = data.get("languages", [])
    work_language = ", ".join([lang["key"].replace("/languages/", "") for lang in languages]) if languages else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    # ‚úÖ 2. –ë–µ—Ä—ë–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    pages = data.get("number_of_pages", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")

    # ‚úÖ 3. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Üí –∏—â–µ–º –≤ edition
    best_edition = get_best_edition(book_key)

    publisher = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
    publish_place = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
    
    if best_edition:
        # ‚úÖ 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–∑—ã–∫ –≤ edition
        edition_languages = best_edition.get("languages", [])
        edition_language = ", ".join([lang["key"].replace("/languages/", "") for lang in edition_languages]) if edition_languages else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

        translated_from = best_edition.get("translated_from", [])
        if translated_from:
            translated_language = ", ".join([lang["key"].replace("/languages/", "") for lang in translated_from])
            edition_language = f"{edition_language}, {translated_language}" if edition_language != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö" else translated_language

        language = work_language if work_language != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö" else edition_language

        # ‚úÖ 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º "number_of_pages"
        if pages == "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
            pages = best_edition.get("number_of_pages", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")

        if pages == "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
            pagination = best_edition.get("pagination", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
            if pagination != "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
                pages = pagination.replace(" p.", "").replace(" pages", "")

        # ‚úÖ 6. –ò–∑–¥–∞—Ç–µ–ª—å
        publishers = best_edition.get("publishers", [])
        publisher = ", ".join(publishers) if publishers else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

        # ‚úÖ 7. –ú–µ—Å—Ç–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (—Ñ–∏–∫—Å –±–∞–≥–∞)
        publish_places = best_edition.get("publish_places", [])
        if isinstance(publish_places, list) and len(publish_places) > 0:
            publish_place = ", ".join([place for place in publish_places if isinstance(place, str)])
        if not publish_place or publish_place == "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
            publish_place = data.get("publish_places", ["–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"])[0]

    # ‚úÖ 8. –ñ–∞–Ω—Ä—ã (subjects)
    subjects = data.get("subjects", [])[:MAX_RUBRICS]
    genres = ", ".join(subjects) if subjects else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
   

    return pages, language, publisher, publish_place, genres

def scrape_page(offset, book_id):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–Ω–∏–≥–∞–º–∏"""
    url = f"{BASE_URL}{offset}"
    print(f"üöÄ –°–∫—Ä–∞–ø–∏–Ω–≥ {url}...")

    response = requests.get(url)

    if response.status_code != 200:
        print(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code} –Ω–∞ {url}")
        return book_id

    data = response.json()
    books = data.get("works", [])

    if not books:
        print("‚ö†Ô∏è –ö–Ω–∏–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –≤–æ–∑–º–æ–∂–Ω–æ, –∏–∑–º–µ–Ω–∏–ª—Å—è —Ñ–æ—Ä–º–∞—Ç API.")
        return book_id

    for book in books:
        try:
            title = book.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
            author = book["authors"][0]["name"] if book.get("authors") else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"
            year = book.get("first_publish_year", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
            book_key = book.get("key", None)
            link = f"https://openlibrary.org{book_key}" if book_key else "–ù–µ—Ç —Å—Å—ã–ª–∫–∏"
            editions = book.get("edition_count", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
            cover_url = f"https://covers.openlibrary.org/b/id/{book['cover_id']}-L.jpg" if book.get("cover_id") else "–ù–µ—Ç –æ–±–ª–æ–∂–∫–∏"
            
            # ‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            pages, language, publisher, publish_place, genres = get_book_details(book_key)

            print(f"‚úÖ {book_id}. {title} ({year}) - {author} | {pages} —Å—Ç—Ä. | {language} | {genres} | –ò–∑–¥–∞—Ç–µ–ª—å: {publisher} | –ü—É–±–ª–∏–∫–∞—Ü–∏—è: {publish_place}")
            RESULTS.append({
                "ID": book_id,
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title,
                "–ê–≤—Ç–æ—Ä": author,
                "–ì–æ–¥": year,
                "–°—Å—ã–ª–∫–∞": link,
                "–ò–∑–¥–∞–Ω–∏–π": editions,
                "–°—Ç—Ä–∞–Ω–∏—Ü": pages,
                "–Ø–∑—ã–∫": language,
                "–ò–∑–¥–∞—Ç–µ–ª—å": publisher,
                "–ú–µ—Å—Ç–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏": publish_place,
                "–ñ–∞–Ω—Ä—ã": genres,
                "–û–±–ª–æ–∂–∫–∞": cover_url
            })
            book_id += 1  
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–Ω–∏–≥–∏: {e}")

    return book_id

def save_data():
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSON, Excel –∏ CSV"""
    if RESULTS:
        df = pd.DataFrame(RESULTS)
        df = df.drop_duplicates()

        df.to_json("opeenlibrary_books.json", orient="records", force_ascii=False, indent=4)
        df.to_excel("opeenlibrary_books.xlsx", index=False)
        df.to_csv("opeenlibrary_books.csv", index=False, encoding="utf-8-sig")

        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã! –ö–Ω–∏–≥ —Å–æ–±—Ä–∞–Ω–æ: {len(df)}")
    else:
        print("\n‚ùå –û—à–∏–±–∫–∞: –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—å OpenLibrary!")

def scrape_all():
    """–ü–∞—Ä—Å–∏–º 5000+ –∫–Ω–∏–≥"""
    book_id = 1
    for page in range(MAX_PAGES):
        offset = page * 100
        book_id = scrape_page(offset, book_id)
        time.sleep(random.randint(*DELAY_RANGE))

    save_data()

# –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
scrape_all()